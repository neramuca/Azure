{"cells":[{"cell_type":"markdown","source":["# High Performance Spark Queries with Databricks Delta\nDatabricks Delta extends Apache Spark to simplify data reliability and boost Spark's performance.\n\nBuilding robust, high performance data pipelines can be difficult due to: _lack of indexing and statistics_, _data inconsistencies introduced by schema changes_ and _pipeline failures_, _and having to trade off between batch and stream processing_.\n\nWith Databricks Delta, data engineers can build reliable and fast data pipelines. Databricks Delta provides many benefits including:\n* Faster query execution with indexing, statistics, and auto-caching support\n* Data reliability with rich schema validation and rransactional guarantees\n* Simplified data pipeline with flexible UPSERT support and unified Structured Streaming + batch processing on a single data source.\n\n### Let's See How Databricks Delta Makes Spark Queries Faster!\n\nIn this example, we will see how Databricks Delta can optimize query performance. We create a standard table using Parquet format and run a quick query to observe its latency. We then run a second query over the Databricks Delta version of the same table to see the performance difference between standard tables versus Databricks Delta tables. \n\nSimply follow these 4 steps below:\n* __Step 1__ : Create a standard Parquet based table using data from US based flights schedule data\n* __Step 2__ : Run a query to to calculate number of flights per month, per originating airport over a year\n* __Step 3__ : Create the flights table using Databricks Delta and optimize the table.\n* __Step 4__ : Rerun the query in Step 2 and observe the latency. \n\n__Note:__ _Throughout the example we will be building few tables with a 10s of million rows. Some of the operations may take a few minutes depending on your cluster configuration._"],"metadata":{}},{"cell_type":"code","source":["%fs rm -r /tmp/flights_parquet "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res0: Boolean = true\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["%fs rm -r /tmp/flights_delta"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res1: Boolean = true\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["flights = spark.read.format(\"csv\") \\\n  .option(\"header\", \"true\") \\\n  .option(\"inferSchema\", \"true\") \\\n  .load(\"/databricks-datasets/asa/airlines/2008.csv\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["flights.write.format(\"parquet\").mode(\"overwrite\").partitionBy(\"Origin\").save(\"/tmp/flights_parquet\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["Once step 1 completes, the \"flights\" table contains details of US flights for a year. \n\nNext in Step 2, we run a query that get top 20 cities with highest monthly total flights on first day of week."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import count\n\nflights_parquet = spark.read.format(\"parquet\").load(\"/tmp/flights_parquet\")\n\ndisplay(flights_parquet.filter(\"DayOfWeek = 1\").groupBy(\"Month\",\"Origin\").agg(count(\"*\").alias(\"TotalFlights\")).orderBy(\"TotalFlights\", ascending=False).limit(20))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Month</th><th>Origin</th><th>TotalFlights</th></tr></thead><tbody><tr><td>6</td><td>ATL</td><td>6046</td></tr><tr><td>3</td><td>ATL</td><td>6019</td></tr><tr><td>12</td><td>ATL</td><td>5800</td></tr><tr><td>9</td><td>ATL</td><td>5722</td></tr><tr><td>6</td><td>ORD</td><td>5241</td></tr><tr><td>3</td><td>ORD</td><td>5072</td></tr><tr><td>9</td><td>ORD</td><td>4931</td></tr><tr><td>7</td><td>ATL</td><td>4894</td></tr><tr><td>8</td><td>ATL</td><td>4821</td></tr><tr><td>4</td><td>ATL</td><td>4798</td></tr><tr><td>11</td><td>ATL</td><td>4776</td></tr><tr><td>10</td><td>ATL</td><td>4684</td></tr><tr><td>5</td><td>ATL</td><td>4656</td></tr><tr><td>2</td><td>ATL</td><td>4601</td></tr><tr><td>1</td><td>ATL</td><td>4540</td></tr><tr><td>12</td><td>ORD</td><td>4473</td></tr><tr><td>7</td><td>ORD</td><td>4249</td></tr><tr><td>8</td><td>ORD</td><td>4171</td></tr><tr><td>4</td><td>ORD</td><td>4140</td></tr><tr><td>5</td><td>ORD</td><td>4134</td></tr></tbody></table></div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["Once step 2 completes, you can observe the latency with the standard \"flights_parquet\" table. \n\nIn step 3 and step 4, we do the same with a Databricks Delta table. This time, before running the query, we run the `OPTIMIZE` command with `ZORDER` to ensure data is optimized for faster retrieval."],"metadata":{}},{"cell_type":"code","source":["flights.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"Origin\").save(\"/tmp/flights_delta\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["display(spark.sql(\"DROP TABLE  IF EXISTS flights\"))\n\ndisplay(spark.sql(\"CREATE TABLE flights USING DELTA LOCATION '/tmp/flights_delta'\"))\n                  \ndisplay(spark.sql(\"OPTIMIZE flights ZORDER BY (DayofWeek)\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th></tr></thead><tbody><tr><td>null</td></tr></tbody></table></div>"]}}],"execution_count":10},{"cell_type":"code","source":["flights_delta = spark.read.format(\"delta\").load(\"/tmp/flights_delta\")\n\ndisplay(flights_delta.filter(\"DayOfWeek = 1\").groupBy(\"Month\",\"Origin\").agg(count(\"*\").alias(\"TotalFlights\")).orderBy(\"TotalFlights\", ascending=False).limit(20))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Month</th><th>Origin</th><th>TotalFlights</th></tr></thead><tbody><tr><td>6</td><td>ATL</td><td>6046</td></tr><tr><td>3</td><td>ATL</td><td>6019</td></tr><tr><td>12</td><td>ATL</td><td>5800</td></tr><tr><td>9</td><td>ATL</td><td>5722</td></tr><tr><td>6</td><td>ORD</td><td>5241</td></tr><tr><td>3</td><td>ORD</td><td>5072</td></tr><tr><td>9</td><td>ORD</td><td>4931</td></tr><tr><td>7</td><td>ATL</td><td>4894</td></tr><tr><td>8</td><td>ATL</td><td>4821</td></tr><tr><td>4</td><td>ATL</td><td>4798</td></tr><tr><td>11</td><td>ATL</td><td>4776</td></tr><tr><td>10</td><td>ATL</td><td>4684</td></tr><tr><td>5</td><td>ATL</td><td>4656</td></tr><tr><td>2</td><td>ATL</td><td>4601</td></tr><tr><td>1</td><td>ATL</td><td>4540</td></tr><tr><td>12</td><td>ORD</td><td>4473</td></tr><tr><td>7</td><td>ORD</td><td>4249</td></tr><tr><td>8</td><td>ORD</td><td>4171</td></tr><tr><td>4</td><td>ORD</td><td>4140</td></tr><tr><td>5</td><td>ORD</td><td>4134</td></tr></tbody></table></div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["The query over the Databricks Delta table runs much faster after `OPTIMIZE` is run. How much faster the query runs can depend on the configuration of the cluster you are running on, however should be **5-10X** faster compared to the standard table."],"metadata":{}}],"metadata":{"name":"High Performance Spark Queries with Databricks Delta (Python)","notebookId":2131966090072088},"nbformat":4,"nbformat_minor":0}