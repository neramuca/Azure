{"cells":[{"cell_type":"code","source":["%run \"./ADP_Logging\""],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["################################################################################\n\"\"\"General Utils.\n\n\"\"\"\n #Who                 When           What\n #Ana Perez           02/10/2018     Initial version\n #Victor Salesa Sanz  18/02/2018     Added saveAsCanonical Function\n #Victor Salesa Sanz  25/02/2018     Unified import libraries aEndnd added new\n #Victor Salesa Sanz  29/19/2018     SliceDFColumn: Optimized to avoid using UDF's\n #Victor Salesa Sanz  02/11/2018     Added isDate & isDigit functions optimized for pyspark\n #Victor Salesa Sanz  06/11/2018     Included sync files before executing shutil operations to avoid unsync errors\n #Victor Salesa Sanz  06/11/2018     Added destName to move files functions (copyFile,distributeFile,....)\n #Victor Salesa Sanz  07/11/2018     Add Window functions library\n #Victor Salesa Sanz  15/11/2018     Add pyspark.sql.functions as psf for ParseJSONCols compatibility\n #                                   Add unpivot function\n #                                   Add ParseJSONCols  function\n #                                   Add flatten_struct function\n #Victor Salesa Sanz  20/11/2018     ParseJSONCols: Add debug Paramater to ParseJSONCols\n #                                   ParseJSONCols: Drop Malformed _ERR fields (no data)\n #Victor Salesa Sanz  26/11/2018     Added jaydebeapi library to perform UPDATE operation that it's not allowed with default spark\n #                                   Added urllib library to encode folder path for \"update\" operations on partitioning    \n #Ana Perez           08/02/2019     Added traceback library and register blob_delete_file_sql\n #Victor Salesa       08/02/2019     Moved blob_delete_file_sql register to UTL_Blob\n #Victor Salesa       11/02/2019     Commented quinn libary as It was conflicting list code and added specific code from quinn linary we were using\n #                                         import functools that is contained into quinn  \n #Victor Salesa       01/03/2019    Create GenDataFrameAsSchema and CopyNullableStateFromSchema to adapt schemas\n #Victor Salesa       02/03/2019    Modify GenDataFrameAsSchema to avoid copying schema\n #Victor Salesa       02/03/2019    Move pyspark import to Utl gen \n #Victor Salesa       27/02/2019    SaveAsCanonical: Changed function to write to parquet\n #Victor Salesa       06/03/2019    SaveAsCanonical: Changed function to write to hive-parquet\n #Ana Perez           07/03/2019    SaveAsCanonical: Moved save operation from if debug==True inside\n #Victor Salesa       08/03/2019    SaveAsCanonical: Added lower() to check table exists as tables query return names in lower case\n #Victor Salesa       14/03/2019    GetFirstLine: Added function\n #Ana Perez           19/03/2019    Added sys library to exception managment\n #Victor Salesa       28/03/2019    Replace sqlContext.refreshTable with sql.catalog.refreshTable\n #Victor Salesa       04/03/2019    Added callSafe method\n #Victor Salesa       10/04/2019    GetFirstLine: Added charset detection in order to be able to read all files and forcing encode enabled.\n #Victor Salesa       10/04/2019    getEncoding: Initial Version\n #Victor Salesa       14/04/2019    Added getNextJobId,getBasePath,RollbackCanonicalTable\n #Victor Salesa       14/04/2019    SaveAsCanonical: Changed saveAsCanonical to include Rollback adn process Id\n #Victor Salesa       17/04/2019    saveToDB: ADDED ROLLBACK ID\n #Victor Salesa       17/04/2019    created RollbackDBTable\n #Victor Salesa       21/04/2019    getNextJobId: Moved id generation to DBVictor Salesa       21/04/2019    Add Start AdpProcess and End AdpProcess\n #Victor Salesa       21/04/2019    Add StartAdpProcess and EndAdpProcess\n #Victor Salesa       29/04/2019    AdpProcess: Changed code to use CTL.SP_GET_JOB STORED PROCEDURE\n################################################################################\n\nfrom datetime import datetime \nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.window import Window\nfrom functools import *\nfrom pprint import pprint\n\n#Quinn library transform\nfrom pyspark.sql.dataframe import DataFrame\ndef transform(self,f):\n  return f(self)\nDataFrame.transform = transform\n################################################################################\n\nfrom multiprocessing.pool import ThreadPool\nfrom itertools import chain\nimport time\nimport urllib\nimport traceback\nimport sys\nfrom pyspark import StorageLevel\n\n#Import functions library as psf for ParseJSONCols compatibility\nimport pyspark.sql.functions  as psf\n\nimport ast  #Abstract Syntax Trees\nimport os    #execute sh commands\nimport numpy as np \nimport shutil #move files library\n\nfrom pyspark.sql import Row\nimport cchardet as chardet\nimport math\nimport jaydebeapi\nimport decimal\n\n\n#Define general Settings for the app\n\n__PARTITIONS_DEFAULT__      = 50\n__DEBUG_DEFAULT__           = False\n__SAMPLE_DEFAULT__          = False\n__SAMPLE_QUANTITY_DEFAULT__ = 100\n\n__YYYMMDD__ = \"%Y%m%d\"\n__YYYYMMDD_p2__ = \"%Y-%m-%d\"\n__YYYYMMDDhhmmss__ = \"%Y%m%d%H%M%S\"\n\n\n################################################################################### \n\ndef GetDataFrameAsSchema(df,source_schema,debug=False,copy_schema=True):\n  \"\"\"Takes the proper fields from df casts the values to the proper types and sets the nullable value. \n     Field names from source_schema should exist in df\n      Parameters:\n      Dataframe df      -- Dataframe with the proper structure\n      debug             -- Enable debug\n      copy_schema       -- Cast value types to schema ones \n    Return:\n      Dataframe -- new dataframe with the changed structure\n\n    Example:\n\n  \"\"\"\n  # Autogenerated select-cast operation to put fields in order and be sure that they have the right schema.\n  if copy_schema==True:\n    generated_select = [col(field.name).cast(field.dataType) for field in source_schema]\n  else:\n    generated_select = [col(field.name) for field in source_schema]\n    \n  df_selected = df.select(*iter(generated_select))\n  \n  if copy_schema==True:\n    df_adapted = CopyNullableStateFromSchema(df_selected,source_schema,debug=debug)\n  else:\n    df_adapted = df_selected\n    \n  return df_adapted\n\n################################################################################### \n\ndef CopyNullableStateFromSchema(df,source_schema,debug=False):\n  \"\"\"Changes nullable state of a dataframe based on a source schema\n\n      Parameters:\n      Dataframe df      -- Dataframe with the proper structure\n    Return:\n      Dataframe -- new dataframe with the changed structure\n\n    Example:\n\n  \"\"\"\n  #Who                 When           What\n  #Victor Salesa       20/11/2018     Initial version\n  source_schema_fields    = [field.name for field in source_schema]\n  for struct_field in df.schema:\n      if struct_field.name in source_schema_fields:\n          if debug==True:\n            print(struct_field.name+\" is \"+str(struct_field.nullable))\n          struct_field.nullable = source_schema[struct_field.name].nullable\n          if debug==True:\n            print(struct_field.name+\" changes to \"+str(struct_field.nullable))\n  if debug==True:\n    print(\"Schema Modified: \"+str(df.schema))\n  df_mod = spark.createDataFrame(df.rdd, df.schema)\n  return df_mod\n\n################################################################################### \n\ndef callSafe(object,method,default,*args):\n  \"\"\"Call Method from Object safely to Avoid NoneType Error\n    Parameters:\n      object:  Object containing the method to be called\n      method:  Name of the method to be called\n      default: Default value when Object is None\n      *args:   Args to be passed to the method in \"method\" parameter (Optional)\n        \n    Return:\n       result of calling object.method(*args)\n  \"\"\"\n  #Who                 When           What\n  #Victor Salesa       04/04/2019     Initial version\n  \n  #Check if method is one of method names\n  try:\n    if method in [method_name for method_name in dir(object)]:\n      #Check if any arg\n      if len(args)!=0:\n        #Run method with Args\n        return getattr(object, method)(*args)\n      else:\n        #Run method without Args\n        return getattr(object, method)()\n      #end if len(args)!=0:\n    else:\n      #Run default value when method is not available\n      return default\n    #end if method in [method_name for method_name in dir(object)]\n  except Exception as err:\n    return default\n    \n  \n################################################################################### \ndef parseJSONCols(df, *cols, sanitize=True,debug=False):\n  \"\"\"Auto infer the schema of a json column and parse into a struct.\n    rdd-based schema inference works if you have well-formatted JSON,\n    like ``{\"key\": \"value\", ...}``, but breaks if your 'JSON' is just a\n    string (``\"data\"``) or is an array (``[1, 2, 3]``). In those cases you\n    can fix everything by wrapping the data in another JSON object\n    (``{\"key\": [1, 2, 3]}``). The ``sanitize`` option (default True)\n    automatically performs the wrapping and unwrapping.\n\n    The schema inference is based on this\n    `SO Post <https://stackoverflow.com/a/45880574)/>`_.\n    Written by Nolan Conaway:\n    `SO Post https://stackoverflow.com/a/51072232)/>`_. \n  \n      Parameters:\n      df : pyspark dataframe\n          Dataframe containing the JSON cols.\n      *cols : string(s)\n          Names of the columns containing JSON.\n      sanitize : boolean\n          Flag indicating whether you'd like to sanitize your records\n          by wrapping and unwrapping them in another JSON object layer.\n      debug: boolean\n          Flag to include debug information or Not\n      \n      Return:\n      pyspark dataframe\n        A dataframe with the decoded columns.\n  \"\"\"\n  #Who                 When           What\n   #Victor Salesa Sanz  15/11/2018     Initial version\n   #Victor Salesa Sanz  20/11/2018     Add debug Paramater to ParseJSONCols\n   #                                   Drop Malformed _ERR fields (no data)\n  ################################################################################\n\n  try:\n    res = df\n    \n    empty_schema = StructType([])\n    \n    if(debug==True):\n      print(\"parseJSONCols: Start Col iteration \")\n  \n    for i in cols:\n\n        if(debug==True):\n            print(\"parseJSONCols: Reading col: \"+str(i))\n          \n        # sanitize if requested.\n        if sanitize:\n            res = (\n                res.withColumn(\n                    i,\n                    psf.concat(psf.lit('{\"data\": '), i, psf.lit('}'))\n                )\n            )\n            \n        if(debug==True):\n          print(\"parseJSONCols: Read from json string\")\n          collect = res.rdd.map(lambda x: x[i]).collect()\n          print(\"parseJSONCols: collect:\"+str(collect))\n        \n        # infer schema and apply it\n        schema = spark.read.json(res.rdd.map(lambda x: x[i]),mode='DROPMALFORMED').schema\n        \n        #Check if schema is not empty\n        if schema!=empty_schema:\n          if(debug==True):\n            print(\"parseJSONCols: Read Schema\")\n            collect = res.rdd.map(lambda x: x[i]).collect()\n            print(\"parseJSONCols: schema:\"+str(schema))\n\n          if(debug==True):\n            print(\"parseJSONCols: Convert json to map\")\n\n          res = res.withColumn(i, psf.from_json(psf.col(i), schema))\n\n        if(debug==True):\n           print(\"parseJSONCols: Sanitize Unpack\")\n        # unpack the wrapped object if needed\n\n        if sanitize:\n            if schema!=empty_schema:\n              #Sanitize column\n              if(debug==True):\n                 print(\"parseJSONCols: Sanitize: \"+str(i))\n              \n              res = res.withColumn(i, psf.col(i).data)\n            else:\n              if(debug==True):\n                 print(\"parseJSONCols: Drop: \"+str(i))\n              #Drop Error column if not valid\n              res = res.drop(i)\n        \n    return res\n  except Exception as e:\n    print('cols cannot be parsed: ' + str(e))\n    return res\n\n################################################################################\n\ndef unpivot(df, by):\n  \"\"\"Unpivot columns in \"df\" parameter excluding columns in \"by\" parameter\n      Written by Zero323:\n          `SO Post https://stackoverflow.com/a/37865645)/>`_.\n\n        Parameters:\n        df -- Dataframe containing columns to unpivot\n        by -- list of columns to be excluded from the unpivot\n\n        Return:\n          Dataframe with the columns unpivoted\n\n        Example:\n          from pyspark.sql.functions import array, col, explode, struct, lit\n          input_df = sc.parallelize([(1, 0.0, 0.6), (1, 0.6, 0.7)]).toDF([\"A\", \"col_1\", \"col_2\"])\n          output_df = unpivot(df, [\"A\"])\n          \n          input_df show:\n            +---+-----+-----+\n            |  A|col_1|col_2|\n            +---+-----+-----+\n            |  1|  0.0|  0.6|\n            |  1|  0.6|  0.7|\n            +---+-----+-----+\n            \n          output_df show\n            +---+-----+---+\n            |  A|  key|val|\n            +---+-----+---+\n            |  1|col_1|0.0|\n            |  1|col_2|0.6|\n            |  1|col_1|0.6|\n            |  1|col_2|0.7|\n            +---+-----+---+\n  \"\"\"\n  #Who                 When           What\n  #Victor Salesa       15/11/2018     Initial version\n  try:\n    \n    # Filter dtypes and split into column names and type description\n    cols, dtypes = zip(*((c, t) for (c, t) in df.dtypes if c not in by))\n    \n    # Spark SQL supports only homogeneous columns\n    assert len(set(dtypes)) == 1, \"All columns have to be of the same type\"\n    \n    # Create and explode an array of (column_name, column_value) structs\n    kvs = explode(array([\n      struct(lit(c).alias(\"key\"), col(c).alias(\"val\")) for c in cols\n    ])).alias(\"kvs\")\n    \n    return df.select(by + [kvs]).select(by + [\"kvs.key\", \"kvs.val\"])\n\n  except Exception as e:\n    print('Df cannot be unpivoted: ' + str(e))\n    return df\n  \n################################################################################\ndef flatten_struct(schema, prefix=\"\"):\n  \"\"\"Takes in a StructType schema object and return a column selector that flattens the Struct\n      Written by Zz'Rot:\n          `SO Post https://stackoverflow.com/a/46942723)/>`_.\n\n        Parameters:\n          schema -- Schema of the Dataframe containing the columns to be flattened\n          prefix -- Prefix to be added to the columns\n\n        Return:\n          Flattened Schema\n\n        Example:\n          df = sc.parallelize([Row(r=Row(a=1, b=Row(foo=\"b\", bar=\"12\")))]).toDF()\n          input_df.show()\n\n          df_expanded = df.select(\"r.*\")\n          df_flattened = df_expanded.select(flatten_struct(df_expanded.schema))\n\n          input_df show:\n            +----------+\n            |         r|\n            +----------+\n            |[1,[12,b]]|\n            +----------+\n\n          df_flattened show\n            +---+-----+-----+\n            |  a|b.bar|b.foo|\n            +---+-----+-----+\n            |  1|   12|    b|\n            +---+-----+-----+\n  \"\"\"\n  #Who                 When           What\n  #Victor Salesa       15/11/2018     Initial version\n\n  try:\n    result = []\n    for elem in schema:\n        if isinstance(elem.dataType, StructType):\n            result += flatten_struct(elem.dataType, prefix + elem.name + \".\")\n        else:\n            result.append(col(prefix + elem.name).alias(prefix + elem.name))\n    return result\n  except Exception as e:\n    print('Schema cannot be flattened')\n    return result\n  \n################################################################################  \n\n\ndef udf_isDate_Def(date_text, my_format): \n  \"\"\"Validate if a string has date format.\n\n      Parameters:\n      date_text -- string with the value to validate\n      my_format -- date format expected\n\n      Return:\n        True: the date format is correct\n        False: the date format is not correct\n        \n      UdfName:\n        udf_isDate\n\n      Example:\n        udf_isDate('2018-02-22', __YYYMMDD__)\n        %sql select udf_isDate('2018-02-22',  \"%Y-%m-%d\")\n\n  \"\"\"\n\n  #Who                 When           What\n  #Ana Perez           02/10/2018     Initial version\n  try:\n      if date_text != datetime.strptime(date_text, my_format).strftime(my_format):\n          return False\n      return True\n  except Exception as e:\n      return False\n      \n#register function for %sql\nspark.udf.register(\"udf_isDate_sql\", udf_isDate_Def, BooleanType())\n#register function for %phyton %scala\nudf_isDate_sql = udf(lambda d, f: udf_isDate_Def(d,f),BooleanType())\n\n################################################################################\n\ndef isDate(date,format='yyyy-mm-dd'):\n  \"\"\"Validate if a string has date format.\n\n      Parameters:\n      date -- string with the date to validate\n      format -- date format expected\n\n      Return:\n        True: the date format is correct\n        False: the date format is not correct\n        \n      Example:\n        spark.createDataFrame([('19970228220707',)], ['t']).select((isDate('t','yyyyMMddhhmmss')).alias('is_date')).collect()\n\n  \"\"\"\n  #Who                 When           What\n  #Victor Salesa       02/11/2018     Initial version\n  return to_date(date,format).isNotNull()\n\n################################################################################\n\ndef udf_slices_Def(s, *args):\n  \n  \"\"\"Split a string in multiple substrings.\n\n      Parameters:\n      s     -- string to be splitted\n      *args -- list of sizes for each substring\n\n      Return:\n        List of strings\n        \n      UdfName:\n        udf_slices_sql\n\n      Example 1:\n        %sql select udf_slices_sql('testingthestring',  \"[5,4,7]\")\n        Out: [\"testi\",\"ngth\",\"estring\"] \n\n      Example 2:\n        my_file = spark.read.text(\"/FileStore/tables/quijote.txt\")\n        my_file.select(udf_slices_sql('value', lit(\"[10,10,12]\"))).show() \n        Out:\n           [Yo, Juan , Gallo de A, ndrada e...|\n           [los que r, esiden en , su Conse...|\n\n  \"\"\"\n  #Who                 When           What\n  #Victor Salesa       03/10/2018     Initial version\n  position = 0\n  myList = []\n  for length in args:\n      myList.append(s[position:position + length])\n      position += length\n  return myList\n  \n#register function for %sql\nspark.udf.register(\"udf_slices_sql\", lambda x, y: udf_slices_Def(x,*ast.literal_eval(y)), ArrayType(StringType()))\n#register function for %phyton %scala\nudf_slices_sql = udf(lambda x, y: udf_slices_Def(x,*ast.literal_eval(y)), ArrayType(StringType()))\n\n################################################################################\n\ndef udf_isDigit_Def(value):\n  \"\"\"Check if a string is a Digit String.\n\n      Parameters:\n      value -- string to checked\n\n      Return:\n        Boolean\n        \n      UdfName:\n        udf_isDigit_sql\n\n      Example 1:\n        %sql select udf_isDigit_sql('12345678')\n        Out: True \n\n      Example 2:\n        my_file = spark.read.text(\"/FileStore/tables/numbers.txt\")\n        my_file.select(\"value\",udf_isDigit_sql('value').alias('isDigit')).show()  \n        Out:\n            +------+-------+\n            | value|isDigit|\n            +------+-------+\n            |123454|   true|\n            |Lalala|  false|\n            | 13223|   true|\n            | Lal24|  false|\n            | 33524|   true|\n            +------+-------+\n  \"\"\"\n  #Who                 When           What\n  #Victor Salesa       03/10/2018     Initial version\n  if value:\n    return value.isdigit()\n  else:\n    return False\n  \n#register function for %sql\nspark.udf.register(\"udf_isDigit_sql\",lambda x: udf_isDigit_Def(x),BooleanType())\n#register function for %phyton %scala\nudf_isDigit_sql = udf(lambda x: udf_isDigit_Def(x),BooleanType())\n\n################################################################################\n\ndef isDigit(column):\n  \"\"\"Check if a column is a Digit String. (just for pyspark)\n\n      Parameters:\n      column -- column to be checked\n\n      Return:\n        Boolean\n\n      Example 1\n        spark.createDataFrame([('123456789045442305823234582349058934',)], ['a']).select(isDigit('a').alias('r')).collect() \n  \"\"\"\n  #Who                 When           What\n  #Victor Salesa       02/11/2018     Initial Version\n  #Victor Salesa       09/01/2019     Added condition length(column)>0 to avoid emtpy strings considered as digit\n  \n  return ((length(translate(column, \"1234567890\", \"\"))==0)&(length(column)>0))\n\n################################################################################\n\ndef distributeFile_Def(sourcefile,destpath,errorpath,isOk,destName=None):\n  \"\"\"Distribute file from sourcefile to destpath if isOk=True or errorPath if isOk=False\n\n      Parameters:\n        sourcefile                  -- file with path to be distributed\n        destpath                    -- destination path when file is ok\n        errorpath                   -- destionation path when file is not ok\n        isOK                        -- the file is moved to destpath if ok or errorpath if not ok\n        destName                    -- file name in destination folder (In case renaming)\n      Return\n        pyspark.sql.Dataframe\n        \n      Example 1:\n        \n  \"\"\"  \n  #Who                 When           What\n  #Victor Salesa       15/10/2018     Initial version\n  #Victor Salesa       06/11/2018     Added destName to move files functions\n  \n  sourcepath_split = sourcefile.split('/')\n  \n  name = sourcepath_split[len(sourcepath_split)-1]\n  \n  if destName!=None:\n      destname = destName\n  else:\n      destname = name\n  \n  sourcefile = '/dbfs' + sourcefile.replace('dbfs:','')  \n  destpath  = '/dbfs' + destpath.replace('dbfs:','') + destname\n  errorpath = '/dbfs' + errorpath.replace('dbfs:','') + destname\n  \n  moveOk =\"ok\"\n  \n  if isOk == True:\n    if len(destpath)!=0:\n      os.system(\"sync %s\" % sourcefile.replace(name,\"\"))\n      os.system(\"sync %s\" % destpath.replace(name,\"\"))\n      moveOk = shutil.move(sourcefile,destpath)\n      # Force filesync in dbfs\n      os.system(\"sync %s\" % sourcefile.replace(name,\"\"))\n      os.system(\"sync %s\" % destpath.replace(name,\"\"))\n  else:\n    if len(errorpath)!=0:\n      # Force filesync in dbfs\n      os.system(\"sync %s\" % sourcefile.replace(name,\"\"))\n      os.system(\"sync %s\" % errorpath.replace(name,\"\"))\n      moveOk = shutil.move(sourcefile,errorpath)\n      # Force filesync in dbfs\n      os.system(\"sync %s\" % sourcefile.replace(name,\"\"))\n      os.system(\"sync %s\" % errorpath.replace(name,\"\"))\n  return (len(moveOk)>0)\n\n#register function for %sql\nspark.udf.register(\"distributeFile_sql\",lambda s,d,e,ok,dest: distributeFile_Def(s,d,e,ok,dest),BooleanType())\n#register function for %phyton %scala\ndistributeFile_sql = udf(lambda s,d,e,ok,dest: distributeFile_Def(s,d,e,ok,dest),BooleanType())\n\n#####################################################################################\n\ndef copyFile_Def(sourcefile,destpath,destName=None):\n  \"\"\"Copy file from sourcefile to destpath\n\n      Parameters:\n        sourcefile                  -- file with path to be distributed\n        destpath                    -- destination path when file is ok\n        destName                    -- file name in destination folder (In case renaming)\n        \n      Return\n        Boolean                     -- file has been moved correctly          \n        \n      Example 1:\n        \n  \"\"\"  \n  #Who                 When           What\n  #Victor Salesa       15/10/2018     Initial version\n  #Victor Salesa       06/11/2018     Added destName to move files functions\n  \n  sourcepath_split = sourcefile.split('/')\n  \n  name = sourcepath_split[len(sourcepath_split)-1]\n  \n  if destName!=None:\n      destname = destName\n  else:\n      destname = sourcepath_split[len(sourcepath_split)-1]\n  \n  sourcefile = '/dbfs' + sourcefile.replace('dbfs:','')  \n  destpath  = '/dbfs' + destpath.replace('dbfs:','') + destname\n  \n  # Force filesync in dbfs\n  os.system(\"sync %s\" % sourcefile.replace(name,\"\"))\n  os.system(\"sync %s\" % destpath.replace(name,\"\"))\n  \n  copyOk = shutil.copy(sourcefile,destpath)\n  \n  # Force filesync in dbfs\n  os.system(\"sync %s\" % sourcefile.replace(name,\"\"))\n  os.system(\"sync %s\" % destpath.replace(name,\"\"))\n  \n  return (len(copyOk)>0)\n\n#register function for %sql\nspark.udf.register(\"copyFile_sql\",lambda s,d,dest: copyFile_Def(s,d,dest),BooleanType())\n#register function for %phyton %scala\ncopyFile_sql = udf(lambda s,d,dest: copyFile_Def(s,d,dest),BooleanType())\n\n################################################################################\ndef deleteFile_Def(pathFile, nameFile):\n  \"\"\"Delete file \n\n      Parameters:\n        pathFile                    -- Path to be deleted\n        nameFile                    -- Name of the file to be deleted\n      Return\n        Boolean                     -- file has been deleted correctly          \n        \n      Example 1:\n        \n  \"\"\"  \n  #Who                 When           What\n  #Ana Perez           15/10/2018     Initial version\n  #Victor Salesa       06/11/2018     Added destName to move files functions\n  \n  try:\n      \n    pathFileTr = '/dbfs' + pathFile.replace('dbfs:','')\n#     print (pathFileTr+nameFile )\n\n    if os.path.exists(pathFileTr+nameFile):\n      os.remove(pathFileTr+nameFile)\n      os.system(\"sync %s\" % pathFileTr)\n    #end if path.exists\n    \n    return True\n      \n  except ValueError:\n    return False\n  \n#register function for %sql\nspark.udf.register(\"deleteFile_sql\",lambda s,d: deleteFile_Def(s,d),BooleanType())\n#register function for %phyton %scala\ndeleteFile_sql = udf(lambda s,d: deleteFile_Def(s,d),BooleanType())\n\n################################################################################\n\n@udf\ndef getEncoding(filePath,debug=True):\n  \"\"\"Get File Charset information\n\n      Parameters:\n        filePath                    -- Path of the file to be read\n        debug                       -- Show debug information\n      Return\n        String                     -- json encoded charset information      \n        \n      Example 1:\n        \n  \"\"\"  \n  #Who                 When           What\n  #Victor Salesa       10/03/2019     Initial version\n  try:\n    filePath = \"/dbfs\"+filePath.replace(\"dbfs:\",\"\")\n    \n    exception_state = ''\n    with open(filePath,'rb') as f:\n      if debug==True:\n        print(\"[\"+datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')+\"]\"+\" Read First Line\")\n      #end if debug==True\n      \n      FirstLineBytes = f.readline()\n      exception_state = 'getEncoding.READ_LINE'\n      \n      if debug==True:\n        print(\"[\"+datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')+\"]\"+\" Detect Encoding\")\n      #end if debug==True\n      charset =    chardet.detect(FirstLineBytes)['encoding']\n      confidence = chardet.detect(FirstLineBytes)['confidence']\n      \n      if debug==True:\n        print(\"[\"+datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')+\"]\"+\" Before Json\")\n      \n      charset_desc = '\"charset\"'\n      confidence_desc = '\"confidence\"'\n      charset_json = '\"'+charset+'\"'\n      confidence_json = str(confidence)\n      \n      encoding_det_json = '{'+charset_desc+\":\"+charset_json+\",\"+confidence_desc+\":\"+confidence_json+'}'\n  \n      if debug==True:\n        print(\"[\"+datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')+\"]\"+\" After Json\")\n#       print(encoding_det_json)\n      exception_state = 'getEncoding.DETECT_ENCODING'\n      return encoding_det_json\n    #end with open(filePath,'rb') as f\n    \n   \n  except Exception as e:\n    print(\"[\"+datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')+\"]\"+\" Fail on \"+exception_state+\": \"+str(e))\n    return \"\"\n#end def getEncoding(filePath,debug=True):\n\n################################################################################\n@udf\ndef getFirstLine(filePath,force_decode=\"\",debug=True):\n  \"\"\"Get first line of the file in the path \n\n      Parameters:\n        filePath                    -- Path of the file to be read\n        debug                       -- Show debug information\n      Return\n        String                     -- first line of the file      \n        \n      Example 1:\n        \n  \"\"\"  \n  #Who                 When           What\n  #Victor Salesa       13/03/2019     Initial version\n  #Victor Salesa       10/04/2019     Added charset detection in order to be able to read all files.\n  try:\n    filePath = \"/dbfs\"+filePath.replace(\"dbfs:\",\"\")\n    \n    with open(filePath,'rb') as f:\n      if debug==True:\n        print(\"[\"+datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')+\"]\"+\" Read First Line\")\n      #end if debug==True\n      \n      FirstLineBytes = f.readline()\n      exception_state = 'getFirstLine.READ_LINE'\n      \n      if debug==True:\n        print(\"[\"+datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')+\"]\"+\" Detect Encoding\")\n      #end if debug==True\n      charset =    chardet.detect(FirstLineBytes)['encoding']\n      confidence = chardet.detect(FirstLineBytes)['confidence']\n      \n      exception_state = 'getFirstLine.DETECT_ENCODING'\n    #end with open(filePath,'rb') as f\n    \n    if debug==True:\n      print(\"[\"+datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')+\"]\"+\" Decode First Line\")\n      print(\"\\n\"+\"encoding: \"+charset+\" | confidence:\"+str(confidence))\n      print(\"-----------------------\")\n    #end if debug==True\n    \n    if len(force_decode)!=0:\n      decode_charset = force_decode\n    else:\n      decode_charset = charset\n    #end if len(force_decode)!=0\n    \n    FirstLineDecodedString = FirstLineBytes.decode(charset)\n    exception_state = 'getFirstLine.DECODE_LINE'\n\n    if debug==True:\n      print(\"[\"+datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')+\"]\"+\" Remove CRLF\")\n    #end if debug==True\n\n    FirstLineDecodedStringRemoveCRLF = FirstLineDecodedString.rstrip()\n    exception_state = 'getFirstLine.STRIP_LINE'\n\n#     print(\"[\"+datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')+\"]\"+\" First Line: \"+str(FirstLineDecodedStringRemoveCRLF))\n    return FirstLineDecodedStringRemoveCRLF\n  except Exception as e:\n    if debug==True:\n      print(\"[\"+datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')+\"]\"+\"Fail on \"+exception_state+\": \"+str(e))\n    return \"\"\n#end def getFirstLine(filePath,debug=True):\n\n\n################################################################################\n\ndef createDFFromList(filelist):\n  \"\"\"Create a Dataframe based on filelist.\n\n      Parameters:\n      filelist                  -- python list of StructType that contains a pair key-value with name     \n      \n      Return:\n        pyspark.sql.Dataframe\n        \n      Example 1:\n        filelist = dbutils.fs.ls(\"wasbs://container1@adppocstoragev2.blob.core.windows.net/01-Landing\")\n        createDFFromList(filelist).show()\n        \n  \"\"\" \n  #Who                 When           What\n  #Victor Salesa       03/10/2018     Initial version\n  \n  return sc.parallelize(filelist).toDF()\n\n################################################################################################################################################\ndef saveToDB(df,table_name,mode=\"overwrite\",alternate_db_url='',debug=False,job_id=''):\n  \"\"\"Save Dataframe in Database\n\n        Parameters:\n        df               -- dataframe to be saved\n        mode             -- save mode overwrite/append/....\n        table_name       -- hive table name. If not specified or \"\" or None will take file name of destination path\n        alternate_db_url -- if db url is not default \n        Return:\n          Boolean\n\n        Example 1:\n          result = saveToDB(df,\"T_SELL_OUT\",mode=\"overwrite\")\n  \"\"\"\n  #Who                 When           What\n  #Victor Salesa       14/04/2019     Add job id to save Todb\n  \n  exception_status = \"saveToDB.START_PROCESS\"\n\n  df = df.cache()\n  df.count()\n\n  if job_id !='':\n    if not job_id.isdigit():\n      raise Exception(\"job_i should be an integer\")\n\n    #Append a job id column to the dataframe \n    df = df.withColumn(\"JOB_ID\",lit(job_id).cast(DecimalType(31,0)))\n    df = df.cache()\n    df.count()\n  #end if job_id !='':\n\n  exception_status = \"saveToDB.PROCESS_ID_ADDED\"\n  \n  if alternate_db_url!=None and len(alternate_db_url.strip())!=0:\n     current_jdbcurl = alternate_db_url\n  else:\n     current_jdbcurl = __JDBC_URL__\n  try:\n    #Write to Database\n    if debug==True:\n      print(\"[\"+datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')+\"]\"+\"Save to \"+table_name+\" with mode \"+mode)\n    (df.write\n       .mode(mode)\n       .jdbc(current_jdbcurl,table_name)\n    )\n    if debug==True:\n      print(\"[\"+datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')+\"]\"+'Write to db table:'+str(table_name))\n      df.show(10000,truncate=False)\n    \n    #DROP Field \n    if job_id !='':\n      df = df.drop(\"JOB_ID\")\n      df = df.cache()\n      df.count()\n    #end if job_id !='':\n    \n  except Exception as e:\n    if job_id!='':\n      RollbackDBTable(table_name,rollback_id=job_id)\n    #end if job_id!=''\n    raise e\n  \n########################################################################################################################\ndef StartADPProcess(process_name,debug=False):\n  \"\"\"Generates a new PID Start for the current Process \n\n        Parameters:\n          process_name - Name of the process we are starting\n          debug -- Enable debug for the current process\n        StartADPProcess\n        Return:\n          String\n          \n        Example 1:\n          result = StartADPProcess('process_name')\n  \"\"\"\n  #Who                 When           What\n  #Victor Salesa       18/04/2019     Initial Version\n  #Victor Salesa       29/04/2019     Changed code to use CTL.SP_GET_JOB STORED PROCEDURE\n  \n  try:\n    \n    ADP_log_info(process, logger_name, level_action, log_level, \"BEGIN\", sys._getframe().f_code.co_name)\n    \n    start_date = datetime.fromtimestamp(time.time()).strftime(__YYYYMMDDhhmmss__)\n    query      = '''DECLARE @result SMALLINT, @sequence INT, @errorCode INT, @errorMsg NVARCHAR(4000);\n                    EXECUTE @result = [CTL].[SP_GET_JOB] NULL, '{0}', '{1}', NULL, NULL, NULL, @out_job_id = @sequence OUTPUT, @out_error_code = @errorCode OUTPUT, @out_error_msg = @errorMsg OUTPUT;\n                    SELECT @result, @sequence, @errorCode, @errorMsg\n                 '''.format(start_date,process_name)\n    \n    exception_status = 'StartADPProcess.START_PROCESS'\n   \n    db = jaydebeapi.connect(__JDBC_CLASSNAME__, __JAYDEBE_URL__)\n    \n    exception_status = \"StartADPProcess.DB_CONNECT\"\n    \n    curs = db.cursor()\n    \n    exception_status = \"StartADPProcess.GET_CURSOR\"\n    \n    execution_result = curs.execute(query)\n    \n    exception_status = \"StartADPProcess.EXECUTE_QUERY\"\n    \n    query_result_array = curs.fetchone()\n    \n    query_result = query_result_array[0]\n    job_id = query_result_array[1]\n    errorCode = query_result_array[2]\n    errorMessage = query_result_array[3]\n    \n    message = \"\"\n    \n    if query_result != 1:\n      query = \" \".join(query.split()).replace(\"; \",\";\").replace(\";\",\";\\n\")\n      message = 'Stored Procedure Execution Failed:||\"'+query+'\"||errorCode:'+str(errorCode)+'|errorMessage:'+str(errorMessage)\n      message = \" \".join(message.split())\n      message = message.replace(\"|\",\"\\n\")\n      curs.close()\n      raise Exception('Stored Procedure Execution Failed: \\n'+message)\n    #end if query_result != 1\n    curs.close()\n    \n    exception_status = \"StartADPProcess.JOB_OPENED\"\n    \n    ADP_log_info(process, logger_name, level_action, log_level, \"END\", sys._getframe().f_code.co_name)\n    \n    return job_id\n  except Exception as e:\n    ADP_log_exception(process, logger_name, level_action, log_level,message, sys._getframe().f_code.co_name,  sys.exc_info())\n    raise e\n    \n########################################################################################################################\n\ndef EndADPProcess(rows_target,job_id='',log_error_code='NULL',log_error_message='',debug=False):\n  \"\"\"Generates a new PID Stop for the current Process \n\n        Parameters:\n          job_id - job id that it's ending\n          rows_target   - Rows finally written to file / database\n          debug -- Enable debug for the current process\n        EndADPProcess\n        Return:\n          String\n          \n        Example 1:\n          result = EndADPProcess('process_name',11111111111111111,5556666)\n  \"\"\"\n  #Who                 When           What\n  #Victor Salesa       18/04/2019     Initial Version\n  #Victor Salesa       29/04/2019     Changed code to use CTL.SP_GET_JOB STORED PROCEDURE\n  \n  try:\n    ADP_log_info(process, logger_name, level_action, log_level, \"BEGIN\", sys._getframe().f_code.co_name)\n    \n    message = \"\"\n    \n    end_date = datetime.fromtimestamp(time.time()).strftime(__YYYYMMDDhhmmss__)\n    query      = '''DECLARE @result SMALLINT, @sequence INT, @errorCode INT, @errorMsg NVARCHAR(4000);\n                    EXECUTE @result = [CTL].[SP_GET_JOB] {0}, '{1}', NULL, {2}, {3}, '{4}', @out_job_id = @sequence OUTPUT, @out_error_code = @errorCode OUTPUT, @out_error_msg = @errorMsg OUTPUT;\n                    SELECT @result, @sequence, @errorCode, @errorMsg\n                 '''.format(job_id,end_date,rows_target,log_error_code,log_error_message)\n\n    exception_status = 'EndADPProcess.START_PROCESS'\n   \n    db = jaydebeapi.connect(__JDBC_CLASSNAME__, __JAYDEBE_URL__)\n    \n    exception_status = \"EndADPProcess.DB_CONNECT\"\n    \n    curs = db.cursor()\n    \n    exception_status = \"EndADPProcess.GET_CURSOR\"\n    \n    execution_result = curs.execute(query)\n    \n    exception_status = \"EndADPProcess.EXECUTE_QUERY\"\n    \n    query_result_array = curs.fetchone()\n    \n    query_result = query_result_array[0]\n    job_id = query_result_array[1]\n    errorCode = query_result_array[2]\n    errorMessage = query_result_array[3]\n    \n\n    \n    if query_result != 1:\n      query = \" \".join(query.split()).replace(\"; \",\";\").replace(\";\",\";\\n\")\n      message = 'Stored Procedure Execution Failed:||\"'+query+'\"||errorCode:'+str(errorCode)+'|errorMessage:'+str(errorMessage)\n      message = \" \".join(message.split())\n      message = message.replace(\"|\",\"\\n\")\n      curs.close()\n      raise Exception('Stored Procedure Execution Failed: \\n'+message)\n    #end if query_result != 1\n    curs.close()\n    \n    exception_status = \"EndADPProcess.JOB_CLOSED\"\n    \n    ADP_log_info(process, logger_name, level_action, log_level, \"END\", sys._getframe().f_code.co_name)\n    \n    return job_id\n  except Exception as e:\n    ADP_log_exception(process, logger_name, level_action, log_level,message, sys._getframe().f_code.co_name,  sys.exc_info())\n    raise e\n\n########################################################################################################################    \n    \ndef getNextJobId(process):\n  \"\"\"Get Job Next Id\n        Parameters:\n        process     -- dataframe to be saved\n\n        Return:\n          Integer\n\n        Example 1:\n          job_id = getNextJobId('PROCESS_NAME')\n  \"\"\"\n    \n  #Who                 When           What\n  #Victor Salesa       12/04/2019     Initial version\n  #Victor Salesa       21/04/2019     Moved Job Id Generation to DB\n  try:\n    \n    ADP_log_info(process, logger_name, level_action, log_level, \"BEGIN\", sys._getframe().f_code.co_name)\n    \n    query = '''DECLARE @result SMALLINT, @sequence INT, @errorCode INT, @errorMsg NVARCHAR(4000);\n               EXECUTE @result = [CTL].[SP_GET_JOB_SEQ] @out_sequence = @sequence OUTPUT, @out_error_code = @errorCode OUTPUT, @out_error_msg = @errorMsg OUTPUT;\n               SELECT @result, @sequence, @errorCode, @errorMsg\n            '''\n    \n    db = jaydebeapi.connect(__JDBC_CLASSNAME__, __JAYDEBE_URL__)\n    \n    exception_status = \"getNextJobId.CONNECT_DATABASE\"\n    \n    curs = db.cursor()\n    \n    exception_status = \"getNextJobId.GET_CURSOR\"\n    \n    result = curs.execute(query)\n    \n    exception_status = \"getNextJobId.EXECUTE_QUERY\"\n    \n    query_result_array = curs.fetchone()\n    \n    exception_status = \"getNextJobId.GET_ROW\"\n  \n    query_result = query_result_array[0]\n    job_id = query_result_array[1]\n    errorCode = query_result_array[2]\n    errorMessage = query_result_array[3]\n    \n    if query_result != 0:\n      query = \" \".join(query.split()).replace(\"; \",\";\").replace(\";\",\";\\n\")\n      message = 'Stored Procedure Execution Failed:||\"'+query+'\"||errorCode:'+str(errorCode)+'|errorMessage:'+str(errorMessage)\n      message = \" \".join(message.split())\n      message = message.replace(\"|\",\"\\n\")\n      curs.close()\n      raise Exception('Stored Procedure Execution Failed: \\n'+message)\n    #end if query_result != 0\n    \n    curs.close()\n    \n    ADP_log_info(process, logger_name, level_action, log_level, \"END\", sys._getframe().f_code.co_name)\n    \n    return job_id\n    #return math.floor(time.time() * 1000000000000000000000)\n  except Exception as e:\n    ADP_log_exception(process, logger_name, level_action, log_level,message, sys._getframe().f_code.co_name,  sys.exc_info())\n    raise e\n\n################################################################################################################################################\n    \n@udf\ndef getBasePath(file):\n  \"\"\"BasePath of the file provided\n\n        Parameters:\n        file\n        \n        Return:\n          string\n  \"\"\"\n    \n  #Who                 When           What\n  #Victor Salesa       19/10/2018     Initial version\n  \n  try:\n    return '/'.join((file.split('/'))[:-1])\n  except Exception as e:\n    return file\n  \n################################################################################################################################################\n\ndef RollbackDBTable(table_name,rollback_id=''):\n  \"\"\"Rollback a datanase table if process id is defined in the table\n\n      Parameters:\n      table_name  -- hive table name.\n      rollback_id -- id of the process id data\n      Return:\n        Integer\n   \"\"\"\n\n  #Who                 When           What\n  #Victor Salesa       15/04/2019     Initial version\n  ADP_log_info(process, logger_name, level_action, log_level, \"BEGIN\", sys._getframe().f_code.co_name)\n  \n  query_delete = '''DELETE FROM {0} WHERE JOB_ID={1}'''.format(table_name,rollback_id)\n  \n  query_delete_tmpt = '''DECLARE @result SMALLINT, @rowsDeleted INT, @errorCode INT, @errorMsg NVARCHAR(4000);\n                        EXECUTE @result = [CTL].[SP_DEL_JOB_ID] '{0}', {1}, @out_rows_deleted = @rowsDeleted OUTPUT, @out_error_code = @errorCode OUTPUT, @out_error_msg  = @errorMsg OUTPUT;\n                        SELECT @result, @rowsDeleted, @errorCode, @errorMsg\n                     '''\n  exception_status = ''\n  \n  try:\n    \n    if not rollback_id.isdigit():\n      raise Exception(\"rollback_id should be an integer\")\n    #end if not rollback_id.isdigit()\n    \n    query_delete = query_delete_tmpt.format(table_name,rollback_id)\n  \n    exception_status = \"RollbackDBTable.CHECK_ROLLBACK_ID\"\n    \n    db = jaydebeapi.connect(__JDBC_CLASSNAME__, __JAYDEBE_URL__)\n\n    exception_status = \"RollbackDBTable.CONNECT_DATABASE\"\n  \n    curs = db.cursor()\n    \n    exception_status = \"RollbackDBTable.GET_CURSOR\"\n\n    execution_result = curs.execute(query_delete)\n    \n    query_result_array = curs.fetchone()\n    \n    query_result = query_result_array[0]\n    rowsDeleted = query_result_array[1]\n    errorCode = query_result_array[2]\n    errorMessage = query_result_array[3]\n    \n    exception_status = \"RollbackDBTable.EXECUTE_QUERY\"\n    \n    if query_result !=0:\n      query_delete = \" \".join(query_delete.split()).replace(\"; \",\";\").replace(\";\",\";\\n\")\n      message = 'Stored Procedure Execution Failed:||\"'+query_delete+'\"||errorCode:'+str(errorCode)+'|errorMessage:'+str(errorMessage)\n      message = \" \".join(message.split())\n      message = message.replace(\"|\",\"\\n\")\n      curs.close()\n      raise Exception('Stored Procedure Execution Failed: \\n'+message)\n    # end if query_result !=0:\n    curs.close()\n    exception_status = \"RollbackDBTable.ROLLBACK_DB_TABLE\"\n    \n    if rowsDeleted <= 0:\n      ADP_log_warning(process, logger_name, level_action, log_level, \"Job id {0} doesn't exist in table {1}\".format(str(rollback_id),table_name), sys._getframe().f_code.co_name)\n  \n    ADP_log_info(process, logger_name, level_action, log_level, \"END\", sys._getframe().f_code.co_name)\n  \n    return rowsDeleted\n  except Exception as e:\n    ADP_log_exception(process, logger_name, level_action, log_level,message, sys._getframe().f_code.co_name,  sys.exc_info())\n    raise e\n\n################################################################################################################################################\n\ndef RollbackCanonicalTable(table_name,rollback_id='latest'):\n  \"\"\"Rollback a canonical table if process id is defined in the table\n\n      Parameters:\n      table_name  -- hive table name.\n      rollback_id -- id of the partition to be removed if latest is set then will remove the biggest number id\n      Return:\n        Boolean\n   \"\"\"\n\n  #Who                 When           What\n  #Victor Salesa       15/04/2019     Initial version\n  #Victor Salesa       02/05/2019     Avoid fail when id not exists\n  try:\n    ADP_log_info(process, logger_name, level_action, log_level, \"BEGIN\", sys._getframe().f_code.co_name)\n    \n    if rollback_id=='latest':\n      if table_name.lower() in sqlContext.tableNames(\"default\"):\n        if \"PROCESS_ID\" in spark.read.table(table_name).columns:\n          rollback_id = (spark.read.table(table_name).select(\"PROCESS_ID\")\n                                                 .distinct()\n                                                 .orderBy(\"PROCESS_ID\",ascending=True)\n                                                 .groupBy()\n                                                 .agg({\"PROCESS_ID\": \"max\"})\n                                                 .collect()[0][0])\n        else:\n          raise Exception(\"PROCESS_ID field doesn't exit in table: \"+table_name)\n        #end if table_name.lower() in sqlContext.tableNames(\"default\")\n      else:\n        raise Exception(\"Table name:\"+table_name+\" doesn't exit\")\n      #end if table_name.lower() in sqlContext.tableNames(\"default\"):  \n    else: \n      if not rollback_id.isdigit():\n        raise Exception(\"rollback_id should be an integer\")\n      #end if not rollback_id.isdigit()\n    #end if rollback_id='latest':\n        \n    if table_name.lower() in sqlContext.tableNames(\"default\"):\n      rollback_count = spark.read.table(table_name).filter(col(\"PROCESS_ID\")==rollback_id).count()\n      \n      ADP_log_debug(process, logger_name, level_action, log_level, \"Rollback count:\"+str(rollback_count), sys._getframe().f_code.co_name)\n\n      if rollback_count > 0:\n        rollback_path = (spark.read.table(table_name)\n                           .filter(col(\"PROCESS_ID\")==rollback_id)\n                           .select(getBasePath(input_file_name()).alias(\"ROLLBACK_PATH\"))\n                        ).distinct().collect()[0].ROLLBACK_PATH\n        \n        ADP_log_debug(process, logger_name, level_action, log_level, \"Rollback path:\"+rollback_path, sys._getframe().f_code.co_name)\n        \n        rollback_files = sc.parallelize(blob_ls(rollback_path)).toDF().select(\"path\").withColumn(\"deleted\",blob_delete_file_sql(col(\"path\")))\n\n        ADP_log_debug(process, logger_name, level_action, log_level, \"Files about to be deleted\", sys._getframe().f_code.co_name)\n        \n        deleted_files = rollback_files.collect()\n\n        ADP_log_debug(process, logger_name, level_action, log_level, \"Files deleted: \" + str(deleted_files), sys._getframe().f_code.co_name)\n        \n        spark.catalog.refreshTable(table_name)\n\n        ADP_log_debug(process, logger_name, level_action, log_level, \"Table Refreshed: \" + table_name, sys._getframe().f_code.co_name)\n      else:\n        ADP_log_warning(process, logger_name, level_action, log_level, \"No data for the rollback_id: \"+str(rollback_id)+\"in table \"+table_name, sys._getframe().f_code.co_name)\n      #end if rollback_count > 0\n      \n    else:\n      raise Exception(\"Table name:\"+table_name+\" doesn't exit\")\n    #end table_name.lower() in sqlContext.tableNames(\"default\")\n    \n    ADP_log_info(process, logger_name, level_action, log_level, \"END\", sys._getframe().f_code.co_name)\n  except Exception as e:\n    ADP_log_exception(process, logger_name, level_action, log_level, \"\", sys._getframe().f_code.co_name,  sys.exc_info())\n    raise e\n\n################################################################################################################################################\n\ndef saveAsCanonical(df,path,mode=\"overwrite\",table_name=\"\",field_partitions=[],debug=False,job_id=''):\n   #Save the datraframe as a file\n    \"\"\"Save Dataframe in Canonical Format\n\n        Parameters:\n        df          -- dataframe to be saved\n        path        -- path to store the file\n        mode        -- save mode overwrite/append/....\n        table_name  -- hive table name. If not specified or \"\" or None will take file name of destination path\n\n        Return:\n          Boolean\n\n        Example 1:\n          result = saveAsCanonical(df,__PHARMATIC_MASTER_DATA_CANONICAL_PRODUCTS_PATH__,mode=\"overwrite\")\n    \"\"\"\n    \n    #Who                 When           What\n    #Victor Salesa       19/10/2018     Initial version\n    #Victor Salesa       19/11/2018     Removed repartition (1) from canonical\n    #Victor Salesa       27/02/2019     Changed function to write to parquet\n    #Victor Salesa       06/03/2019     Changed function to write to hive-parquet\n    #Ana Perez           07/03/2019     Moved save operation from if debug==True inside\n    #Victor Salesa       08/03/2019     Added lower() to check table exists as tables query return names in lower case\n    #Victor Salesa       27/03/2019     Move the refresh table to the right position\n    #Victor Salesa       09/04/2019     Added \"and mode!='overwrite'\" to force overwrite mode saves data in blob instead of hive metastore\n    #Victor Salesa       12/04/2019     Add a new job id column to the dataframe before writing if specified\n    #Victor Salesa       15/04/2019     Added rollback function call to remove all data regarding the id \n    try:\n      #As the list object is mutable we should create a \"local\" copy of the list to be soure we are not modifying the original object\n      field_partitions = field_partitions[:]\n      \n      exception_status = \"saveAsCanonical.START_PROCESS\"\n\n      df = df.cache()\n      df.count()\n      \n      if job_id !='':\n        if not job_id.isdigit():\n          raise Exception(\"job_i should be an integer\")\n          \n        #Append a job id partition to be able to rollback\n        field_partitions.append(\"PROCESS_ID\")\n\n        #Append a job id column to the dataframe \n        df = df.withColumn(\"PROCESS_ID\",lit(job_id))\n        df = df.cache()\n        df.count()\n      #end if job_id !='':\n      \n      exception_status = \"saveAsCanonical.PROCESS_ID_ADDED\"\n      \n      if table_name==None or len(table_name.strip())==0:\n        table_name=(path.split('/'))[-1]\n        if debug==True:\n          print(\"[\"+datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')+\"]\"+\" Table name not specified defaulted to '\"+table_name+\"'\")\n      else:\n        if debug==True:\n          print(\"[\"+datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')+\"]\"+\" Table name is '\"+table_name+\"'\")\n         \n      #if exists table append or overwrite\n      if table_name.lower() in sqlContext.tableNames(\"default\") and mode!='overwrite':\n        if debug==True:\n          print(\"[\"+datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')+\"]\"+\" table '\"+table_name+\"' exists\")\n          \n        if len(field_partitions)==0:  \n          (df.write\n            .saveAsTable(table_name, mode=mode) \n          )\n          if debug==True:\n            print(\"[\"+datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')+\"]\"+'Write to canonical table:'+str(table_name)+' path: '+str(path))\n            df.show(10000,truncate=False)\n        else:\n          (df.write\n            .partitionBy(*iter(field_partitions))\n            .saveAsTable(table_name, mode=mode) \n          )\n          if debug==True:\n            print(\"[\"+datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')+\"]\"+'Write to canonical table:'+str(table_name)+' path: '+str(path))\n            df.show(10000,truncate=False)\n      else:\n        if debug==True:\n          print(\"[\"+datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')+\"]\"+\" table '\"+table_name+\"' doesn't exist\")\n        if len(field_partitions)==0:\n          #if does not exists table, create\n          (df.write\n            .option('path',path)\n            .saveAsTable(table_name, mode=mode) \n          )\n          if debug==True:\n            print(\"[\"+datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')+\"]\"+'Write to canonical table:'+str(table_name)+' path: '+str(path))\n            df.show(10000,truncate=False)\n        else:\n          (df.write\n            .partitionBy(*iter(field_partitions))\n            .option('path',path)\n            .saveAsTable(table_name, mode=mode) \n          )\n      spark.catalog.refreshTable(table_name)\n      if debug==True:\n        print(\"[\"+datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')+\"]\"+'Refresh canonical table:'+str(table_name))\n      \n      #DROP Field \n      if job_id !='':\n        df = df.drop(\"PROCESS_ID\")\n        df = df.cache()\n        df.count()\n      #end if job_id !='':\n      return True\n    except Exception as e:\n      print(\"[\"+datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')+\"]\"+\" Error Trying to save: \"+path+\" - Exception was: \" + str(e) )\n      ADP_log_exception(process, logger_name, level_action, log_level, \" Error Trying to save: \"+path+\" - Exception was: \", sys._getframe().f_code.co_name,  sys.exc_info())\n      #If writing process fails and any register in canonical table exists it will be deleted and table refreshed\n      if job_id!='':\n        RollbackCanonicalTable(table_name,rollback_id=job_id)\n      #end if job_id!='':\n      \n      \n      traceback.print_exc()\n      raise e\n    \n################################################################################\n\ndef SliceDFColumn(column_to_split_name,splitted_column_names,splitted_column_lengths,purge = '',debug=False):\n  def inner(df):\n    \"\"\"Slice df column based on column names list and lengths.\n\n\n        Parameters:\n        column_to_split_name      -- column name containing the file name\n        splitted_column_names     -- list with column names to be produced2  \n        splitted_column_lengths   -- list of widths for the column names to be producted\n        debug                     -- enable debug\n        \n        Return:\n          pyspark.sql.DataFrame\n\n        Example 1:\n\n          __PH_UNIQUE_CODE_LEN__  = 5; __SPEC_VER_LEN__        = 3; __RELEASE_VERSION_LEN__ = 2   \n          __DATE_LEN__            = 8; __ORIGIN_LEN__          = 1; __FILE_TYPE_LEN__       = 2\n\n          __SP_FILENAME_LENGHTS__        = [__PH_UNIQUE_CODE_LEN__,__SPEC_VER_LEN__,__RELEASE_VERSION_LEN__,__DATE_LEN__,__ORIGIN_LEN__,__FILE_TYPE_LEN__]\n          __SP_FILENAME_COLUMN_NAMES__   = ['pharmacy_unique_code','spec_version','release_version','data_date','origin','file_type']\n          __SP_FILENAME_POSITIONS__      = [pos for pos in range(len(__SP_FILENAME_COLUMN_NAMES__))]\n\n          filelist = dbutils.fs.ls(\"wasbs://container1@adppocstoragev2.blob.core.windows.net/01-Landing\")\n\n          filelist_df = createDFFromList(filelist)\n\n          SliceDFColumn(filelist_df,\"name\",__SP_FILENAME_COLUMN_NAMES__,__SP_FILENAME_LENGHTS__,'SELL-OUT_').show()\n\n          +--------------------+------------+---------------+---------+------+---------+\n          |pharmacy_unique_code|spec_version|release_version|data_date|origin|file_type|\n          +--------------------+------------+---------------+---------+------+---------+\n          |               60552|         293|             01| 20171106|     G|       SL|\n          +--------------------+------------+---------------+---------+------+---------+        \n\n    \"\"\"  \n    if debug==True:\n      print(\"[\"+datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')+\"]\"+ '---------------------------Start Slicing----------------------------------------')\n    \n    start_list = np.cumsum([1] + splitted_column_lengths).tolist()[:-1]\n    \n    splitted_columns = ([\n      col(column_to_split_name).substr(\n          start_list[i], \n          splitted_column_lengths[i]\n      ).alias(splitted_column_names[i]) for i in range(len(start_list))\n    ])\n    \n    return(\n      df.withColumn(column_to_split_name,regexp_replace(column_to_split_name,purge,''))\n        .select(expr(\"*\"),*splitted_columns)\n    )\n    \n  return inner\n\n##################################################################################################################################################################\n##################################################################################################################################################################\n##################################################################################################################################################################\n##################################################################################################################################################################\n\n# Define validation functions\ndef no_val(val):\n  \"\"\"Placeholder Function to be used in a Validation Model to include a Dummy Field with \"No Validation\" \n    Parameters:\n      val    -- column to be validated\n    Return:\n      Boolean\n\n    Example 1:\n     For examples on this function please see \"GenerateValCols\"\n  \"\"\"\n  #Who                 When           What\n  #Victor Salesa       23/01/2019     Initial Version\n  \n  return lit(True);\n\n##################################################################################################################################################################\n\ndef digit_val(val):\n  \"\"\"Function to be used in a Validation Model to perform \"Is Digit Value Validation\" \n    Parameters:\n      val    -- column to be validated\n    Return:\n      Boolean\n      \n    Example 1:\n     For examples on this function please see \"GenerateValCols\"\n  \"\"\"\n  #Who                 When           What\n  #Victor Salesa       23/01/2019     Initial Version\n  \n  return isDigit(val)\n\n##################################################################################################################################################################\n  \ndef date_val_ddmmyy(val):\n  \"\"\"Function to be used in a Validation Model to perform \"Is Date Value with dd/mm/yyyy Validation\" \n    Parameters:\n      val    -- column to be validated\n      \n    Return:\n      Boolean\n\n    Example 1:\n     For examples on this function please see \"GenerateValCols\"\n  \"\"\"\n  #Who                 When           What\n  #Victor Salesa       23/01/2019     Initial Version\n  \n  return isDate(val,format='dd/mm/yy')\n\n##################################################################################################################################################################\n\ndef date_val_yyyymmddHHmmss(val):\n  \"\"\"Function to be used in a Validation Model to perform \"Is Date Value with yyyymmddHHmmss Validation\" \n    Parameters:\n      val    -- column to be validated\n      \n    Return:\n      Boolean\n\n    Example 1:\n     For examples on this function please see \"GenerateValCols\"\n  \"\"\"\n  #Who                 When           What\n  #Victor Salesa       23/01/2019     Initial Version\n  \n  return isDate(val,format='yyyymmddHHmmss')\n\n##################################################################################################################################################################\n  \ndef yesno_val(val):\n  \"\"\"Function to be used in a Validation Model to perform \"Is a Yes or No Valu\n  e\" \n    Parameters:\n      val    -- column to be validated\n      \n    Return:\n      Boolean\n\n    Example 1:\n     For examples on this function please see \"GenerateValCols\"\n  \"\"\"\n  #Who                 When           What\n  #Victor Salesa       23/01/2019     Initial Version\n  \n  return col(val).isin([\"Yes\",\"No\",\"Y\",\"N\"]) \n\n##################################################################################################################################################################\n##################################################################################################################################################################\n##################################################################################################################################################################\n##################################################################################################################################################################"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2}],"metadata":{"name":"UTL_Gen","notebookId":4470134901802159},"nbformat":4,"nbformat_minor":0}
