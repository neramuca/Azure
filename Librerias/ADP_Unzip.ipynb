{"cells":[{"cell_type":"code","source":["%run \"./ADP_Farmatic_Def\""],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["#################################################################################\n\"\"\" Unzip Files Process Functions\n\n\"\"\"\n #Who                 When           What\n #Victor Salesa       12/12/2018     Initial Version\n #Victor Salesa       21/01/2019     Moved patch - depatch ftp lib to functions\n #Victor Salesa       19/02/2019     Change code to retrieve zip password as a parameter\n #Victor Salesa       04/03/2019     Added GPR & GST Housekeeping function to delete all files not GSL\n #Victor Salesa       12/03/2019     udf_unzip: Change unzip process to include ZIP file suffix\n #Victor Salesa       13/03/2019     DeleteAllButSelloutFilesFromLanding: Fix function to select files to delete locating 'GSL' string\n #Victor Salesa       22/03/2019     FMT_FTPTriggerFilesDownload: Initial Version\n #Ana Perez           26/03/2019     Included log managment and exception managment\n #Victor Salesa       04/03/2019     created UnzipPharmaticFilesFromIngestion,MoveUncompressedFilesToArchive,StoreUnzipResultsToCSV \n #                                   for better code understanding and isolated Exception and Error control of the different blocks.\n #Ana Perez           03/05/2019     Split ADP_Farmatic_Ingestion_Process in ADP_FTP and ADP_UNZIP\n #Victor Salesa       07/05/2019     Rename DeleteAllButSelloutFilesFromLanding to  DeleteFilesFromLandingExcludingPattern and add patterns parameter\n################################################################################\n\nimport socket\nfrom pyspark.sql import Row\nfrom dateutil import parser\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom shutil import copyfile\nfrom datetime import datetime\n\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom shutil import *\nimport zipfile\n\n###################################################################################  \n\n@udf \ndef udf_unzip(zip_file,output_path,passwd=None):\n  \"\"\"Unzips a single file in output_path in a spark sql pipeline\n  \n      Parameters:\n        zip_file: Complete Path of the zip file\n        output_path: Output path for uncompressed files\n        passwd: Password of the zip file if it has\n    Return:\n       Return:\n        1  if file unzipped ok\n        0  if file not unzipped ok\n    Example:\n      zip_list = (sc.parallelize(dbutils.fs.ls(\"/mnt/blob/ingestion/pharmatic\")).toDF()\n                    .filter(col(\"name\")\n                    .substr(-3,3)=='ZIP')\n                    .select(regexp_replace(\"path\", 'dbfs:', '/dbfs').alias(\"path\"),col(\"name\"),col(\"size\"))\n                    .withColumn(\"uncompress_result\",unzip(\"path\",lit(\"/dbfs/mnt/blob/landing/pharmatic\")))\n      )\n\n  \"\"\"\n  #Who                 When           What\n  #Victor Salesa       12/12/2018     Initial version\n  #Victor Salesa       19/02/2018     Change code to include zip passwd as a parameter\n  #Victor Salesa       12/03/2019     Change unzip process to include ZIP file suffix\n  #Ana Perez           26/03/2019     Included log managment and exception managment\n  try:\n#     ADP_log_debug(process, logger_name, level_action, log_level, \"BEGIN unzip \"+ zip_file + \" to \" + output_path, sys._getframe().f_code.co_name)\n    \n    #Encode password to pass to unzip library\n    passwd = bytes(passwd.encode(\"UTF-8\"))\n    \n    #Split zip name to get historical files suffix\n    txt_file_sufix_list = zip_file.upper().split('_')\n    \n    #If the file has a suffix we take that suffix otherwhise we use _0101 (1 file of 1)\n    if(len(txt_file_sufix_list)>1):\n      txt_file_sufix = \"_\"+zip_file.upper().split('_')[-1].replace(\".ZIP\",\"\")\n    else:\n      txt_file_sufix = \"_0101\"\n    \n    #Open the zip file\n    with zipfile.ZipFile(zip_file) as myzip:\n      #Create a dict with output names for each txt file inside file.txt --> file+suffix.txt\n      output_file_names_dict = {zip_file_item:zip_file_item.replace(\".TXT\",txt_file_sufix+\".TXT\") for zip_file_item in myzip.namelist()}\n      #Create a dict with creation date for each txt file inside \n      output_file_dates_dict = {zip_file_item.filename:datetime(*zip_file_item.date_time) for zip_file_item in myzip.infolist()}\n      #For each txt file inside\n      for output_file_name in output_file_names_dict.keys():\n        #Open the file\n        with myzip.open(name=output_file_name,pwd=passwd) as myfile:\n          #Create a new file with the renamed name\n          myfile_extracted = open(output_path+output_file_names_dict[output_file_name], \"wb\")\n          #Write file data with data from txt file readed\n          myfile_extracted.write(myfile.read())\n          myfile_extracted.close()\n        # end myzip.open(name=output_file_name,pwd=passwd) as myfile:\n      #end output_file_name in output_file_names_dict.keys():\n    #end with zipfile.ZipFile(zip_file) as myzip\n#     ADP_log_debug(process, logger_name, level_action, log_level, \"END unzip \"+ zip_file + \" to \" + output_path, sys._getframe().f_code.co_name)\n    return 1\n  except Exception as err:\n\n#     ADP_log_debug(process, logger_name, level_action, log_level, \"END Warning Fail unzip \"+ zip_file + \" to \" + output_path, sys._getframe().f_code.co_name)\n    return 0\n  \n###################################################################################\n\ndef UnzipPharmaticFilesFromIngestion(files_list,uncompress_folder,zip_password,debug=True):\n  \"\"\"Trigger unzip Files from Ingestion\n    Parameters:\n      uncompress_folder: folder where to place the unzipped files\n      files_list: Output List of blob_ls with File to be selected to uncompress every list element should be a Row with the columns \"name\",\"path\",\"size\"\n                  \"name\":\"name of the file\"\n                  \"path\":\"full path of the file in dbfs:/\"\n                  \"size\":\"size of the file\"\n      zip_password: zip password for the unzipped files\n    Return:\n       Dataframe: Dataframe with the unzipping results\n  \"\"\"\n  #Who                 When           What\n  #Victor Salesa       02/04/2019     Initial version\n  try:\n    ADP_log_info(process, logger_name, level_action, log_level, \"BEGIN\", sys._getframe().f_code.co_name)    \n    ADP_log_debug(process, logger_name, level_action, log_level, \"Before unzip zip_list in uncompress folder\", sys._getframe().f_code.co_name)\n\n    #Filter zip files to be uncompressed. Uncompress files in folder\n    zip_list_df = (sc.parallelize(files_list).toDF()\n                    .filter(col(\"name\").substr(-3,3)=='ZIP')\n                    .select(\"name\",\"path\",\"size\")\n                    .withColumn(\"raw_path\",regexp_replace(\"path\", 'dbfs:', '/dbfs'))\n                  )\n    #Mark cache in order to cache list and execute\n    zip_list_df.cache()\n    zip_list_count = zip_list_df.count()\n    \n    if zip_list_count!=0:\n      ADP_log_debug(process, logger_name, level_action, log_level, \"Before unzip zip_list in uncompress folder\" , sys._getframe().f_code.co_name)\n      \n      zip_list_df = zip_list_df.withColumn(\"uncompress_result\",udf_unzip(\"raw_path\",lit(uncompress_folder),lit(zip_password)))\n      ADP_log_debug(process, logger_name, level_action, log_level, \"After unzip zip_list in uncompress folder\" , sys._getframe().f_code.co_name)\n      \n      #Cache and execute unzipping operation\n      zip_list_df.cache()\n      zip_list_df.count()\n      \n      #Move uncompressed files to archive folder\n      ADP_log_debug(process, logger_name, level_action, log_level, \"After cache zip_list_df \" + str(zip_list_df.count()), sys._getframe().f_code.co_name)\n      ADP_log_info(process, logger_name, level_action, log_level, \"END\", sys._getframe().f_code.co_name) \n      return zip_list_df\n    else:\n      ADP_log_info(process, logger_name, level_action, log_level, \"END No zip Files\", sys._getframe().f_code.co_name)    \n      return None\n    #end zip_list_count!=0:\n\n  except Exception as e:\n    ADP_log_warning(process, logger_name, level_action, log_level, \"END Fail Unzipping files\", sys._getframe().f_code.co_name)\n    return None\n  #end except Exception as e\n\n##################################################################################################################################################################  \n  \ndef MoveUncompressedFilesToArchive(zip_files_list_df,archiving_folder):\n  \"\"\"Move Uncompressed Zips to Archive\n    Parameters:\n      zip_files_list_df: DataFrame containing Files to be moved with the following structure:\n        \n        \n    Return:\n       Dataframe: Dataframe with the unzipping results\n  \"\"\"\n  #Who                 When           What\n  #Victor Salesa       02/04/2019     Initial version\n  try:\n    ADP_log_info(process, logger_name, level_action, log_level, \"BEGIN\", sys._getframe().f_code.co_name)    \n    ADP_log_debug(process, logger_name, level_action, log_level, \"Before move zip_files_moved with \", sys._getframe().f_code.co_name)\n    \n    zip_files_moved = zip_list_df.rdd.map(lambda r: r+Row(archived_result=('1' if blob_move_file(r.path,archiving_folder) else '0'),archived_file='dbfs:'+archiving_folder.replace('dbfs:','')+r.name)  ).collect()\n\n    #Create dataframe with zips moved\n    zip_files_moved_df = sc.parallelize(zip_files_moved).toDF(zip_list_df.columns + ['archived_result','archived_file'])\n    \n    #Cache and execute moving operation\n    zip_files_moved_df.cache()\n    zip_files_moved_count = zip_files_moved_df.count()\n    \n    ADP_log_debug(process, logger_name, level_action, log_level, \"After create zip_files_moved_df \", sys._getframe().f_code.co_name)\n   \n    #Check Moved Files\n    if zip_files_moved_count!=0:\n      ADP_log_info(process, logger_name, level_action, log_level, \"END\", sys._getframe().f_code.co_name)\n      return zip_files_moved_df\n    else:\n      ADP_log_warning(process, logger_name, level_action, log_level, \"END No files moved\", sys._getframe().f_code.co_name)\n      return None\n  \n  except Exception as e:\n    ADP_log_warning(process, logger_name, level_action, log_level, \"END Fail Moving Zip Files\", sys._getframe().f_code.co_name)\n    return None\n  #end except Exception as e\n  \n##################################################################################################################################################################  \n  \ndef StoreUnzipResultsToCSV(zip_files_moved_df,ingestion_path):\n  \"\"\"Store Unzip Results to CSV\n    Parameters:\n  \"\"\"\n  #Who                 When           What\n  #Victor Salesa       02/04/2019     Initial version\n  try:\n    ADP_log_info(process, logger_name, level_action, log_level, \"BEGIN\", sys._getframe().f_code.co_name)    \n    #Create file with unzipped information\n    (zip_files_moved_df\n         .coalesce(1)\n         .write\n         .option(\"sep\",\"|\")\n         .option(\"sep\",\"|\")\n         .option(\"header\", True)\n         .option(\"QuoteMode\", \"NONE\")\n         .option(\"charset\", \"utf-8\")\n         .mode('overwrite')\n         .csv(__INGESTION_BASE_PATH__+'files_unzipped.csv')\n      )\n\n    # Retrives the internal part-*.csv from \"Spark Way\" csv in order to create a former csv\n    src = [file for file in dbutils.fs.ls(ingestion_path+'files_unzipped.csv/') if 'csv' in file.name]\n    #Generates the former local fs names to move the partition csv file to a new filename\n    src_path = src[0].path.replace('dbfs:/','/dbfs/')\n    src_name = src[0].name\n\n    #Generates a timestamp to mark the unzipping datetime\n    filename_tst = str(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n\n    # Moves the partition file part*.csv to a new file named with the timestamp\n    copyfile(src_path, '/dbfs'+ingestion_path+'files_log/files_unzipped_'+filename_tst+'.csv')\n    ADP_log_info(process, logger_name, level_action, log_level, \"END\", sys._getframe().f_code.co_name) \n  except Exception as e:\n    ADP_log_warning(process, logger_name, level_action, log_level, \"END Fail Generating Unzip results CSV\", sys._getframe().f_code.co_name,  sys.exc_info())\n    return None\n  #end except Exception as e\n  \n  \n ####################################################################################################################################################   \n  \ndef DeleteFilesFromLandingExcludingPattern(patterns=__PHARMATIC_FILES_INCLUDED_PATTERN__,debug=False):\n  \"\"\"Deletes all unzipped Files but Sellout ones from Landing folder\n  \"\"\"\n  \n  #Who                 When           What\n  #Victor Salesa       04/03/2019     Initial version\n  #Victor Salesa       13/03/2019     Fix function to select files to delete locating 'GSL' string\n  #Ana Perez           26/03/2019     Included log managment and exception managment\n  \n  try:\n    ADP_log_info(process, logger_name, level_action, log_level, \"BEGIN\", sys._getframe().f_code.co_name)  \n    \n    ADP_log_debug(process, logger_name, level_action, log_level, \"Select Files\", sys._getframe().f_code.co_name)\n    \n    #Select files to be deleted\n    unzipped_files_not_sellout_selected_df = sc.parallelize(blob_ls(__PHARMATIC_LANDING_BASE_PATH__)).filter(lambda file: all(pattern not in file.name for pattern in patterns)).toDF()\n\n    # ## Delete old Timestamp files from folder as they are not going to be processed\n    unzipped_files_not_sellout_deleted = (unzipped_files_not_sellout_selected_df.select(\"path\")\n                                     .distinct()\n                                     .withColumn(\"deleted\",blob_delete_file_sql(\"path\"))\n                                     .collect()\n    )\n\n    ADP_log_debug(process, logger_name, level_action, log_level, \"Delete Files\", sys._getframe().f_code.co_name)\n    \n    if len(unzipped_files_not_sellout_deleted)!=0:\n\n      ADP_log_debug(process, logger_name, level_action, log_level, \"Get Results\", sys._getframe().f_code.co_name)\n      \n      #Mount a dataframe with deleted files result\n      unzipped_files_not_sellout_deleted_df = sc.parallelize(unzipped_files_not_sellout_deleted).toDF()\n      unzipped_files_not_sellout_deleted_df.cache()\n\n      ADP_log_debug(process, logger_name, level_action, log_level, \"Calculate Deleted\", sys._getframe().f_code.co_name)\n      \n      #Calculate Deleted Files\n      total_deleted = (unzipped_files_not_sellout_deleted_df.agg(sum(\"deleted\").alias(\"deleted_total\")).collect())[0].deleted_total\n                                 \n      ADP_log_debug(process, logger_name, level_action, log_level, \" Deleted Count: \" + str(total_deleted), sys._getframe().f_code.co_name)\n      \n      #Calculate Total Files\n      total_rows = unzipped_files_not_sellout_deleted_df.count()\n\n      ADP_log_debug(process, logger_name, level_action, log_level, \" Row Count: \" + str(total_rows), sys._getframe().f_code.co_name)\n      \n      #Generate Error if total_deleted < total_rows\n      if total_deleted < total_rows:\n          ADP_log_debug(process, logger_name, level_action, log_level, \"Fail Deleting\", sys._getframe().f_code.co_name)\n          \n          not_deleted = unzipped_files_not_sellout_deleted_df.filter(col(\"deleted\")==0).collect()\n          not_deleted_names = str([file.name for file in not_deleted]).replace(\"[\",\"\").replace(\"]\",\"\")\n          ADP_log_exception(process, logger_name, level_action, log_level,  \"Fail deleting bad files: \"+ not_deleted_names, sys._getframe().f_code.co_name,  sys.exc_info())\n          raise Exception(err)\n\n      ADP_log_debug(process, logger_name, level_action, log_level, \"Files Deleted:\" + str(unzipped_files_not_sellout_deleted), sys._getframe().f_code.co_name)\n      \n    ADP_log_info(process, logger_name, level_action, log_level, \"END\", sys._getframe().f_code.co_name)  \n    \n  except Exception as err:\n    ADP_log_exception(process, logger_name, level_action, log_level,  \"Warning: Not Stock or Purchases files found\", sys._getframe().f_code.co_name,  sys.exc_info())\n    raise Exception(err)\n\n    ###################################################################################  "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2}],"metadata":{"name":"ADP_Unzip","notebookId":2882562916532286},"nbformat":4,"nbformat_minor":0}
