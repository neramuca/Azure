{"cells":[{"cell_type":"code","source":["%run \"./UTL_Gen\""],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["#################################################################################\n\"\"\" Set of Functions for Quality Data Structure Generation\n\n\"\"\"\n #Who                 When           What\n #Victor Salesa       15/11/2018     Initial Version\n #Victor Salesa       19/11/2018     QA_CTL_PROCESS_FILE_ERROR_DATA: Replaced Field _Err spec with _ERR spec and _Raw Spec with _RAW spec\n #                                   QA_CTL_PROCESS_FILE_ERROR_DATA: Replaced Field ROW_LINE with FILE_LINE_NUM\n #Victor Salesa       20/11/2018     QA_CTL_PROCESS_FILE_ERROR_DATA: Replaced Field FIELDNAME with FIELD_NAME     \n #                                   QA_CTL_PROCESS_FILE_QA_DATA: Initial Version\n #                                   QA_CTL_PROCESS_FILE_ERROR_DATA: Changed expected Type for LANDING_DATE and PROCESS_DATE to be Timestamp\n #                                   QA_CTL_PROCESS_FILE_QA_DATA: Changed expected Type for LANDING_DATE and PROCESS_DATE to be Timestamp\n #Victor Salesa       22/11/2018     QA_GENERATE_DATA: Added condition to avoid storing Errors when no Errors\n #Ana PÃ©rez           26/02/2019     Adapted to new Temporary source Dataframe \n##################################################################################\n\ndef QA_CTL_PROCESS_FILE_ERROR_DATA(df,debug=False):\n  '''Canonize dataframe to a Data Format to Populate Database for table CTL.PROCESS_FILE\n    \n    Parameters:\n      df: Dataframe with data to be canonized according the following structure\n        Input Dataframe Expected Structure:\n          FILE_NAME :   String\n          LANDING_DATE: Timestamp\n          PROCESS_DATE: Timestamp\n          FILE_LINE_NUM:String\n          ....................\n          ....................\n          ....................\n          <Field1>_RAW: String\n          <Field2>_RAW: String\n          ....................\n          <Fieldn>_RAW: String \n          <Field1>_ERR: String (JSON STRING)\n          <Field2>_ERR: String (JSON STRING)\n          ....................\n          <Fieldn>_ERR: String (JSON STRING)\n      \n      <Fieldn>_ERR Expected Structure (JSON STRING):\n          {ERR_TYPE_1:<Error_Code_1>,ERR_TYPE_2:<Error_Code_2>,....,ERR_TYPE_N:<Error_Code_N>}\n      \n      debug: Show debug information in the process (Default False)\n    \n    Return:\n      Dataframe\n\n    Example:\n      Sample Origin Dataframe for QA\n  \n      input_df = (spark.createDataFrame(\n      [\n        (\"file1_txt\",\"1\",\"aspirinas\",\"bayer\",\"\"\"{\"ERR_TYPE_1\":\"-1\",\"ERR_TYPE_2\":\"0\",\"ERR_TYPE_3\":\"-1\"}\"\"\",\"\"\"{\"ERR_TYPE_4\":\"-1\",\"ERR_TYPE_5\":\"0\",\"ERR_TYPE_6\":\"-1\"}\"\"\"),\n        (\"file2_txt\",\"1\",\"aspirinas\",\"pepito\", \"\"\"{\"ERR_TYPE_4\":\"-1\",\"ERR_TYPE_5\":\"0\",\"ERR_TYPE_6\":\"-1\"}\"\"\",\"\"\"{\"ERR_TYPE_1\":\"-1\",\"ERR_TYPE_2\":\"0\",\"ERR_TYPE_3\":\"-1\"}\"\"\"),\n        (\"file3_txt\",\"1\",\"aspirinas\",\"pepito\", \"\"\"\"\"\",\"\"\"{\"ERR_TYPE_1\":\"-1\",\"ERR_TYPE_2\":\"0\",\"ERR_TYPE_3\":\"-1\"}\"\"\")\n      ],\n      (\"FILE_NAME\",\"ROW_LINE\",\"PRODUCT_RAW\",\"MANUFACTURER_RAW\",\"PRODUCT_ERR\",\"MANUFACTURER_ERR\"))\n      .withColumn(\"temp_timestamp\",from_unixtime(unix_timestamp(current_timestamp())))\n          .withColumn(\"LANDING_DATE\",col(\"temp_timestamp\").cast(StringType()))\n          .withColumn(\"PROCESS_DATE\",col(\"temp_timestamp\").cast(StringType()))\n          .drop(\"temp_timestamp\")\n      )\n      input_df.show(100000,False)\n            \n      #Canonize QA Dataframe\n      qa_df = QA_CTL_PROCESS_FILE_ERROR_DATA(df)\n      \n      qa_df.show(10000,False)\n      \n      input_df Show\n      \n      +---------+-------------+-----------+----------------+------------------------------------------------------+------------------------------------------------------+-------------------+-------------------+\n      |FILE_NAME|FILE_LINE_NUM|PRODUCT_RAW|MANUFACTURER_RAW|PRODUCT_ERR                                           |MANUFACTURER_ERR                                      |LANDING_DATE       |PROCESS_DATE       |\n      +---------+-------------+-----------+----------------+------------------------------------------------------+------------------------------------------------------+-------------------+-------------------+\n      |file1_txt|1            |aspirinas  |bayer           |{\"ERR_TYPE_1\":\"-1\",\"ERR_TYPE_2\":\"0\",\"ERR_TYPE_3\":\"-1\"}|{\"ERR_TYPE_4\":\"-1\",\"ERR_TYPE_5\":\"0\",\"ERR_TYPE_6\":\"-1\"}|2018-11-15 15:26:08|2018-11-15 15:26:08|\n      |file2_txt|1            |aspirinas  |pepito          |{\"ERR_TYPE_4\":\"-1\",\"ERR_TYPE_5\":\"0\",\"ERR_TYPE_6\":\"-1\"}|{\"ERR_TYPE_1\":\"-1\",\"ERR_TYPE_2\":\"0\",\"ERR_TYPE_3\":\"-1\"}|2018-11-15 15:26:08|2018-11-15 15:26:08|\n      |file3_txt|1            |aspirinas  |pepito          |                                                      |{\"ERR_TYPE_1\":\"-1\",\"ERR_TYPE_2\":\"0\",\"ERR_TYPE_3\":\"-1\"}|2018-11-15 15:26:08|2018-11-15 15:26:08|\n      +---------+-------------+-----------+----------------+------------------------------------------------------+------------------------------------------------------+-------------------+-------------------+\n      \n      qa_df Show\n      \n      +---------+-------------+-------------------+-------------------+-------+----------+----------+-----------+\n      |FILE_NAME|FILE_LINE_NUM|LANDING_DATE       |PROCESS_DATE       |FIELD  |ERROR_TYPE|ERROR_CODE|FIELD_VALUE|\n      +---------+-------------+-------------------+-------------------+-------+----------+----------+-----------+\n      |file2_txt|1            |2018-11-15 15:58:00|2018-11-15 15:58:00|PRODUCT|ERR_TYPE_4|-1        |aspirinas  |\n      |file2_txt|1            |2018-11-15 15:58:00|2018-11-15 15:58:00|PRODUCT|ERR_TYPE_4|-1        |aspirinas  |\n      |file2_txt|1            |2018-11-15 15:58:00|2018-11-15 15:58:00|PRODUCT|ERR_TYPE_4|-1        |aspirinas  |\n      |file2_txt|1            |2018-11-15 15:58:00|2018-11-15 15:58:00|PRODUCT|ERR_TYPE_4|-1        |aspirinas  |\n      |file2_txt|1            |2018-11-15 15:58:00|2018-11-15 15:58:00|PRODUCT|ERR_TYPE_4|-1        |aspirinas  |\n      |file2_txt|1            |2018-11-15 15:58:00|2018-11-15 15:58:00|PRODUCT|ERR_TYPE_4|-1        |aspirinas  |\n      |file2_txt|1            |2018-11-15 15:58:00|2018-11-15 15:58:00|PRODUCT|ERR_TYPE_5|0         |aspirinas  |\n      |file2_txt|1            |2018-11-15 15:58:00|2018-11-15 15:58:00|PRODUCT|ERR_TYPE_5|0         |aspirinas  |\n      |file2_txt|1            |2018-11-15 15:58:00|2018-11-15 15:58:00|PRODUCT|ERR_TYPE_5|0         |aspirinas  |\n      |file2_txt|1            |2018-11-15 15:58:00|2018-11-15 15:58:00|PRODUCT|ERR_TYPE_5|0         |aspirinas  |\n      +---------+-------------+-------------------+-------------------+-------+----------+----------+-----------+\n      only showing top 10 rows\n      \n      \n  '''\n  #Who                 When           What\n  #Victor Salesa       15/11/2018     Initial Version\n  #Victor Salesa       19/11/2018     Replaced Field _Err spec with _ERR spec and _Raw Spec with _RAW spec \n  try:\n    ADP_log_info(process, logger_name, level_action, log_level, \"BEGIN\", sys._getframe().f_code.co_name)\n    \n      \n    #Define Key Columns \n    key_columns = [\"FILE_NAME\",\"LANDING_DATE\",\"PROCESS_DATE\",\"FILE_LINE_NUM\",\"FIELD\"]\n  \n    ADP_log_debug(process, logger_name, level_action, log_level, \"---Before iterator error_columns_generator\", sys._getframe().f_code.co_name)\n    \n    error_columns_generator = (col for col in df.schema.names if \"_ERR\" in col)\n    \n    ADP_log_debug(process, logger_name, level_action, log_level, \"---Error Columns Iterator\", sys._getframe().f_code.co_name)\n    not_error_columns_generator = [col for col in df.schema.names if \"_ERR\" not in col]\n    ADP_log_debug(process, logger_name, level_action, log_level, \"---Not Error Columns Iterator\", sys._getframe().f_code.co_name)\n         \n    #Parse from described columns to struct\n    dfsan = parseJSONCols(df,*error_columns_generator,debug=False)\n    ADP_log_debug(process, logger_name, level_action, log_level, \"---Parse from Json\", sys._getframe().f_code.co_name)\n    \n    #Flatten Values to Fields\n    dfsan_flattened = dfsan.select(flatten_struct(dfsan.schema))\n    ADP_log_debug(process, logger_name, level_action, log_level, \"---Flatten Structure\", sys._getframe().f_code.co_name)     \n      \n    #Rename Columns to avoid unpivot problem (. character take as structu)\n    new_column_name_list= list(map(lambda x: x.replace(\".\", \"#\"), dfsan_flattened.columns))\n    dfsan_flattened_renamed = dfsan_flattened.toDF(*new_column_name_list)\n    ADP_log_debug(process, logger_name, level_action, log_level, \"---Rename columns to avoid . error\", sys._getframe().f_code.co_name)\n      \n    #Unpivot Errortypes\n    dfsan_flattened_unpivot = unpivot(dfsan_flattened_renamed,not_error_columns_generator)\n\n    df_table = (dfsan_flattened_unpivot.filter(col(\"val\").isNotNull())\n                          .withColumn('FIELD',regexp_replace(split(\"key\", '#').getItem(0),'_ERR',''))\n                          .withColumn('ERROR_TYPE',split(\"key\", '#').getItem(1))\n                          .withColumn('ERROR_CODE',col(\"val\"))\n                          .drop(\"val\",\"key\")\n    )\n    ADP_log_debug(process, logger_name, level_action, log_level, \"---Unpivot errors\", sys._getframe().f_code.co_name)\n    \n    not_raw_columns_generator = [col for col in df_table.schema.names if \"_RAW\" not in col]\n    df_table_error = df_table.select(*not_raw_columns_generator)\n    ADP_log_debug(process, logger_name, level_action, log_level, \"---Select all but _Raw column\", sys._getframe().f_code.co_name)\n         \n    #Unpivot to add Raw Field Values and Drop Error Related Field \n    df_table_raw = (unpivot(df_table,not_raw_columns_generator)\n                            .drop(\"FIELD\",\"ERROR_TYPE\",\"ERROR_CODE\")\n                            .withColumn(\"FIELD\",regexp_replace(\"key\",\"_RAW\",\"\"))\n                            .withColumn(\"FIELD_VALUE\",col(\"val\"))\n                            .drop(\"key\",\"val\")\n                         ).distinct()\n    ADP_log_debug(process, logger_name, level_action, log_level, \"---Select all but _Err column\", sys._getframe().f_code.co_name)\n      \n    df_table_error = df_table_error.persist(StorageLevel.MEMORY_AND_DISK)\n    df_table_error.count()\n    ADP_log_debug(process, logger_name, level_action, log_level, \"---Cache Error\", sys._getframe().f_code.co_name)\n \n    df_table_raw = df_table_raw.persist(StorageLevel.MEMORY_AND_DISK)\n    df_table_raw.count()\n    ADP_log_debug(process, logger_name, level_action, log_level, \"---Cache RAW\", sys._getframe().f_code.co_name)\n                 \n    \n    join_cond_generated = [ (col('errors.'+field) == col('raw_values.'+field)) for field in key_columns]\n    ADP_log_debug(process, logger_name, level_action, log_level, \"---Generate Join condition\", sys._getframe().f_code.co_name)    \n    \n    df_table_db = df_table_error.alias('errors').join(df_table_raw.alias('raw_values'),join_cond_generated,'left').select(\"errors.*\",\"raw_values.FIELD_VALUE\")\n    ADP_log_debug(process, logger_name, level_action, log_level, \"---Join Error&Raw Data\", sys._getframe().f_code.co_name)\n\n    ADP_log_info(process, logger_name, level_action, log_level, \"END\", sys._getframe().f_code.co_name)  \n    return df_table_db\n  \n  except Exception as err:\n    ADP_log_exception(process, logger_name, level_action, log_level,  \"\", sys._getframe().f_code.co_name,  sys.exc_info())\n    raise Exception(err)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2}],"metadata":{"name":"ADP_QA","notebookId":3996791399039769},"nbformat":4,"nbformat_minor":0}
